{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLbeNiPKJEetMJs47eHL7y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arsendoinychko/Cp-decomposition/blob/main/Untggg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 2\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = CNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
        "            running_loss = 0.0\n",
        "    epoch_duration = time.time() - start_time\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}] completed in {epoch_duration:.2f} seconds')\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')\n",
        "\n",
        "torch.save(model.state_dict(), 'cnn_mnist_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9DG_sfy2z07",
        "outputId": "ed3d92aa-4f61-410b-d185-607679869525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:01<00:00, 6240027.38it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1163915.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 8523962.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 851420.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Step [100/938], Loss: 0.4910\n",
            "Epoch [1/2], Step [200/938], Loss: 0.1123\n",
            "Epoch [1/2], Step [300/938], Loss: 0.0954\n",
            "Epoch [1/2], Step [400/938], Loss: 0.0815\n",
            "Epoch [1/2], Step [500/938], Loss: 0.0788\n",
            "Epoch [1/2], Step [600/938], Loss: 0.0585\n",
            "Epoch [1/2], Step [700/938], Loss: 0.0563\n",
            "Epoch [1/2], Step [800/938], Loss: 0.0641\n",
            "Epoch [1/2], Step [900/938], Loss: 0.0555\n",
            "Epoch [1/2] completed in 140.60 seconds\n",
            "Epoch [2/2], Step [100/938], Loss: 0.0371\n",
            "Epoch [2/2], Step [200/938], Loss: 0.0376\n",
            "Epoch [2/2], Step [300/938], Loss: 0.0353\n",
            "Epoch [2/2], Step [400/938], Loss: 0.0443\n",
            "Epoch [2/2], Step [500/938], Loss: 0.0326\n",
            "Epoch [2/2], Step [600/938], Loss: 0.0445\n",
            "Epoch [2/2], Step [700/938], Loss: 0.0363\n",
            "Epoch [2/2], Step [800/938], Loss: 0.0393\n",
            "Epoch [2/2], Step [900/938], Loss: 0.0417\n",
            "Epoch [2/2] completed in 144.55 seconds\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 98.72%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUEC4fqB26q4",
        "outputId": "c702da51-b49e-4c82-bf43-15d01015a389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorly\n",
            "  Downloading tensorly-0.8.1-py3-none-any.whl (229 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.7/229.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorly) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from tensorly) (1.11.4)\n",
            "Installing collected packages: tensorly\n",
            "Successfully installed tensorly-0.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tensorly.decomposition import parafac\n",
        "import tensorly as tl\n",
        "\n",
        "def cp_decomposition_conv_layer(layer, rank):\n",
        "\n",
        "    S = layer.in_channels\n",
        "    T = layer.out_channels\n",
        "    rank = S*T/(S+T)\n",
        "    rank = round(rank)\n",
        "    print(rank)\n",
        "\n",
        "    weights,(last, first, vertical, horizontal) = parafac(layer.weight.data, rank=rank, init='svd')\n",
        "\n",
        "    pointwise_s_to_r_layer = torch.nn.Conv2d(in_channels=first.shape[0], \\\n",
        "            out_channels=first.shape[1], kernel_size=1, stride=1, padding=0,\n",
        "            dilation=layer.dilation, bias=False)\n",
        "\n",
        "    depthwise_vertical_layer = torch.nn.Conv2d(in_channels=vertical.shape[1],\n",
        "            out_channels=vertical.shape[1], kernel_size=(vertical.shape[0], 1),\n",
        "            stride=1, padding=(layer.padding[0], 0), dilation=layer.dilation,\n",
        "            groups=vertical.shape[1], bias=False)\n",
        "\n",
        "    depthwise_horizontal_layer = \\\n",
        "        torch.nn.Conv2d(in_channels=horizontal.shape[1], \\\n",
        "            out_channels=horizontal.shape[1],\n",
        "            kernel_size=(1, horizontal.shape[0]), stride=layer.stride,\n",
        "            padding=(0, layer.padding[0]),\n",
        "            dilation=layer.dilation, groups=horizontal.shape[1], bias=False)\n",
        "\n",
        "    pointwise_r_to_t_layer = torch.nn.Conv2d(in_channels=last.shape[1], \\\n",
        "            out_channels=last.shape[0], kernel_size=1, stride=1,\n",
        "            padding=0, dilation=layer.dilation, bias=True)\n",
        "\n",
        "    pointwise_r_to_t_layer.bias.data = layer.bias.data\n",
        "\n",
        "    depthwise_horizontal_layer.weight.data = \\\n",
        "        torch.transpose(horizontal, 1, 0).unsqueeze(1).unsqueeze(1)\n",
        "    depthwise_vertical_layer.weight.data = \\\n",
        "        torch.transpose(vertical, 1, 0).unsqueeze(1).unsqueeze(-1)\n",
        "    pointwise_s_to_r_layer.weight.data = \\\n",
        "        torch.transpose(first, 1, 0).unsqueeze(-1).unsqueeze(-1)\n",
        "    pointwise_r_to_t_layer.weight.data = last.unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "    new_layers = [pointwise_s_to_r_layer, depthwise_vertical_layer, \\\n",
        "                    depthwise_horizontal_layer, pointwise_r_to_t_layer]\n",
        "\n",
        "    return nn.Sequential(*new_layers)\n"
      ],
      "metadata": {
        "id": "DJjdmUBV4V3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer = dict(model.named_children())['conv2']"
      ],
      "metadata": {
        "id": "Eyls6SeY4k9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gcZOuw_4oIy",
        "outputId": "ce66889c-a9a5-4118-8ae6-ea6fb3a7d6e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (fc1): Linear(in_features=3136, out_features=1000, bias=True)\n",
            "  (fc2): Linear(in_features=1000, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tl.set_backend('pytorch')"
      ],
      "metadata": {
        "id": "x6BV8GCB4p6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_layers = cp_decomposition_conv_layer(layer, 10)"
      ],
      "metadata": {
        "id": "1RpTWBTc4tLS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "148db7f3-08be-44a8-e043-37a3b03280e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y_eSiQpmkXuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_layers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjAuJtdP4yLH",
        "outputId": "eaaf3bb0-5c17-4bc3-95de-d6f4273cb391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(32, 21, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (1): Conv2d(21, 21, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=21, bias=False)\n",
              "  (2): Conv2d(21, 21, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), groups=21, bias=False)\n",
              "  (3): Conv2d(21, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "setattr(model, 'conv2', nn.Sequential(*new_layers))"
      ],
      "metadata": {
        "id": "eywU95Kc5nTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pySpcYX05qkJ",
        "outputId": "b7fe2fcd-0b6f-4216-cdba-0c53046c9d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(32, 21, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (1): Conv2d(21, 21, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=21, bias=False)\n",
            "    (2): Conv2d(21, 21, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), groups=21, bias=False)\n",
            "    (3): Conv2d(21, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (fc1): Linear(in_features=3136, out_features=1000, bias=True)\n",
            "  (fc2): Linear(in_features=1000, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iyf77Qvk5sOV",
        "outputId": "b7660dbd-5507-4498-f44b-8fa29054ade5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 98.42%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
        "            running_loss = 0.0\n",
        "    epoch_duration = time.time() - start_time\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}] completed in {epoch_duration:.2f} seconds')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6VbjVSk5u9O",
        "outputId": "71cb0f75-1bd6-4241-c2b5-6b5dfd0f1129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Step [100/938], Loss: 0.0284\n",
            "Epoch [1/2], Step [200/938], Loss: 0.0250\n",
            "Epoch [1/2], Step [300/938], Loss: 0.0307\n",
            "Epoch [1/2], Step [400/938], Loss: 0.0274\n",
            "Epoch [1/2], Step [500/938], Loss: 0.0284\n",
            "Epoch [1/2], Step [600/938], Loss: 0.0248\n",
            "Epoch [1/2], Step [700/938], Loss: 0.0188\n",
            "Epoch [1/2], Step [800/938], Loss: 0.0354\n",
            "Epoch [1/2], Step [900/938], Loss: 0.0231\n",
            "Epoch [1/2] completed in 154.62 seconds\n",
            "Epoch [2/2], Step [100/938], Loss: 0.0138\n",
            "Epoch [2/2], Step [200/938], Loss: 0.0172\n",
            "Epoch [2/2], Step [300/938], Loss: 0.0174\n",
            "Epoch [2/2], Step [400/938], Loss: 0.0153\n",
            "Epoch [2/2], Step [500/938], Loss: 0.0174\n",
            "Epoch [2/2], Step [600/938], Loss: 0.0166\n",
            "Epoch [2/2], Step [700/938], Loss: 0.0185\n",
            "Epoch [2/2], Step [800/938], Loss: 0.0206\n",
            "Epoch [2/2], Step [900/938], Loss: 0.0240\n",
            "Epoch [2/2] completed in 157.64 seconds\n",
            "Accuracy of the network on the 10000 test images: 98.97%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchprofile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNzyX-Ih51OO",
        "outputId": "28665172-4cc6-4994-bc08-168322bc3720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchprofile\n",
            "  Downloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.10/dist-packages (from torchprofile) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from torchprofile) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.4 in /usr/local/lib/python3.10/dist-packages (from torchprofile) (0.18.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (4.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.4->torchprofile)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.4->torchprofile)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.4->torchprofile)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.4->torchprofile)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.4->torchprofile)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.4->torchprofile)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.4->torchprofile)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.4->torchprofile)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.4->torchprofile)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.4->torchprofile)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.4->torchprofile)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4->torchprofile)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.4->torchprofile) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4->torchprofile) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4->torchprofile) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchprofile\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 torchprofile-0.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchprofile\n",
        "\n",
        "dummy_input = torch.randn(1, 1, 28, 28)\n",
        "\n",
        "# Обчислення FLOPs\n",
        "flops = torchprofile.profile_macs(model, dummy_input)\n",
        "\n",
        "# Перетворення в GFLOPs\n",
        "gflops = flops / 1e9\n",
        "gflops =gflops*1000\n",
        "\n",
        "print(f'FLOPs in convolutions (Giga): {gflops:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z88YB12vHFqR",
        "outputId": "3b590046-02ee-495b-9249-1743a3ceafe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOPs in convolutions (Giga): 3.57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "\n",
        "# Гіперпараметри\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 2\n",
        "\n",
        "# Вибір пристрою (GPU, якщо доступний, або CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Завантаження та нормалізація набору даних MNIST\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Визначення конволюційної нейронної мережі\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)  # Розгортаємо тензор для повнозв'язного шару\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Ініціалізація моделі, функції втрат та оптимізатора\n",
        "model = CNN().to(device)  # Перенесення моделі на GPU\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Навчання моделі\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()  # Початок відліку часу\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Перенесення даних на GPU\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:  # Кожні 100 міні-батчів\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
        "            running_loss = 0.0\n",
        "    epoch_duration = time.time() - start_time  # Кінець відліку часу\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}] completed in {epoch_duration:.2f} seconds')\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Тестування моделі\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Перенесення даних на GPU\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')\n",
        "\n",
        "# Збереження моделі\n",
        "torch.save(model.state_dict(), 'cnn_mnist_model.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wzmz3yEUHaWx",
        "outputId": "a182187a-eaca-4091-f39e-6bd697b91118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 43493286.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1688137.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 13631078.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 8604574.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Epoch [1/2], Step [100/938], Loss: 0.6337\n",
            "Epoch [1/2], Step [200/938], Loss: 0.1476\n",
            "Epoch [1/2], Step [300/938], Loss: 0.0933\n",
            "Epoch [1/2], Step [400/938], Loss: 0.0831\n",
            "Epoch [1/2], Step [500/938], Loss: 0.0724\n",
            "Epoch [1/2], Step [600/938], Loss: 0.0703\n",
            "Epoch [1/2], Step [700/938], Loss: 0.0688\n",
            "Epoch [1/2], Step [800/938], Loss: 0.0653\n",
            "Epoch [1/2], Step [900/938], Loss: 0.0573\n",
            "Epoch [1/2] completed in 144.91 seconds\n",
            "Epoch [2/2], Step [100/938], Loss: 0.0341\n",
            "Epoch [2/2], Step [200/938], Loss: 0.0392\n",
            "Epoch [2/2], Step [300/938], Loss: 0.0413\n",
            "Epoch [2/2], Step [400/938], Loss: 0.0421\n",
            "Epoch [2/2], Step [500/938], Loss: 0.0420\n",
            "Epoch [2/2], Step [600/938], Loss: 0.0365\n",
            "Epoch [2/2], Step [700/938], Loss: 0.0404\n",
            "Epoch [2/2], Step [800/938], Loss: 0.0465\n",
            "Epoch [2/2], Step [900/938], Loss: 0.0405\n",
            "Epoch [2/2] completed in 157.55 seconds\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 98.98%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchprofile\n",
        "\n",
        "dummy_input = torch.randn(1, 1, 28, 28)\n",
        "\n",
        "# Обчислення FLOPs\n",
        "flops = torchprofile.profile_macs(model, dummy_input)\n",
        "\n",
        "# Перетворення в GFLOPs\n",
        "gflops = flops / 1e9\n",
        "gflops =gflops*1000\n",
        "\n",
        "print(f'FLOPs in convolutions (Giga): {gflops:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z04Wz_P-glhb",
        "outputId": "0d021846-9539-405a-8d5c-a91842706a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOPs in convolutions (Giga): 3.57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Перевірка доступності GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n"
      ],
      "metadata": {
        "id": "6ep9XMqmn-nk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84fb2f20-7f92-464c-ec93-4bcbc2b97cb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import tensorly as tl\n",
        "from tensorly.decomposition import tucker, partial_tucker"
      ],
      "metadata": {
        "id": "bAQqT1geNEKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jz4PowJSG_o4",
        "outputId": "7a713fdf-7637-42ac-a893-b2eec5578e0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorly\n",
            "  Downloading tensorly-0.8.1-py3-none-any.whl (229 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.7/229.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorly) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from tensorly) (1.11.4)\n",
            "Installing collected packages: tensorly\n",
            "Successfully installed tensorly-0.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "core, [last, first] = partial_tucker(tensor, modes=[0,1], ranks=5, init='svd')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "dBXuCiw5HDFH",
        "outputId": "29347487-441b-4b6d-9c54-3269321379fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "partial_tucker() got an unexpected keyword argument 'ranks'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b9c39c761c78>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial_tucker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'svd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: partial_tucker() got an unexpected keyword argument 'ranks'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.randn(2, 3, 4, 5)"
      ],
      "metadata": {
        "id": "muL3T42AHQEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import tensorly as tl\n",
        "from tensorly.decomposition import tucker, partial_tucker\n",
        "\n",
        "def tucker_decomp(layer, rank):\n",
        "    W = layer.weight.data\n",
        "\n",
        "    core, factors = partial_tucker(W, modes=[0,1], rank=rank, init='svd')\n",
        "\n",
        "    last, first, _ = factors\n",
        "\n",
        "    first_layer = nn.Conv2d(in_channels=first.shape[0],\n",
        "                                       out_channels=first.shape[1],\n",
        "                                       kernel_size=1,\n",
        "                                       padding=0,\n",
        "                                       bias=False)\n",
        "\n",
        "    core_layer = nn.Conv2d(in_channels=core.shape[1],\n",
        "                                       out_channels=core.shape[0],\n",
        "                                       kernel_size=layer.kernel_size,\n",
        "                                       stride=layer.stride,\n",
        "                                       padding=layer.padding,\n",
        "                                       dilation=layer.dilation,\n",
        "                                       bias=False)\n",
        "\n",
        "    last_layer = nn.Conv2d(in_channels=last.shape[1],\n",
        "                                       out_channels=last.shape[0],\n",
        "                                       kernel_size=1,\n",
        "                                       padding=0,\n",
        "                                       bias=True)\n",
        "\n",
        "    if layer.bias is not None:\n",
        "        last_layer.bias.data = layer.bias.data\n",
        "\n",
        "    fk = first.t_().unsqueeze_(-1).unsqueeze_(-1)\n",
        "    lk = last.unsqueeze_(-1).unsqueeze_(-1)\n",
        "\n",
        "    first_layer.weight.data = fk\n",
        "    last_layer.weight.data = lk\n",
        "    core_layer.weight.data = core\n",
        "\n",
        "    new_layers = [first_layer, core_layer, last_layer]\n",
        "    return new_layers\n",
        "\n"
      ],
      "metadata": {
        "id": "krsfuVR5HbYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_layers = tucker_decomp(layer, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "wGyfvkNTkYz9",
        "outputId": "4752043d-0546-4fdb-fed6-6b47c94d0b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "tuple index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-61e9bec41a65>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtucker_decomp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-c8e8c714c257>\u001b[0m in \u001b[0;36mtucker_decomp\u001b[0;34m(layer, rank)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     first_layer = nn.Conv2d(in_channels=first.shape[0],\n\u001b[0m\u001b[1;32m     15\u001b[0m                                        \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                        \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W = layer.weight.data"
      ],
      "metadata": {
        "id": "td_2xfOdlapB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ix05vkjL2YFA",
        "outputId": "e0542ac5-35bf-4654-e9a0-1507a73a8df1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 32, 3, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "core, factors = partial_tucker(W, modes=[0,1], rank=30, init='svd')"
      ],
      "metadata": {
        "id": "KXlgFqofmhMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "factors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOnJZCybmnqU",
        "outputId": "2e53d409-7913-4741-e29e-60be35a9d6cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor(0.4314), tensor(0.4311), tensor(0.4311)]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "last, first, _ = factors"
      ],
      "metadata": {
        "id": "pOaHgGxko7M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rse0VZZso7nr",
        "outputId": "9b943b74-cdb7-4bd8-ef7d-91a9ceae4c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorly as tl\n",
        "from tensorly.decomposition import partial_tucker\n",
        "import torch\n",
        "\n",
        "# Створимо приклад 4D тензора\n",
        "shape = (10, 8, 5, 5)  # Приклад розмірів (out_channels, in_channels, height, width)\n",
        "tensor_4d = torch.randn(*shape)  # Перетворення на тензор PyTorch\n",
        "\n",
        "# Виконати частковий Tucker-розклад\n",
        "core, factors = partial_tucker(tensor_4d, modes=[0, 1], rank=[3, 4], init='svd')\n",
        "\n",
        "# Вивести розміри отриманих результатів\n",
        "print(f\"Core shape: {core.shape if isinstance(core, torch.Tensor) else 'Not a tensor'}\")\n",
        "print(f\"Factor shapes: {[factor.shape if isinstance(factor, torch.Tensor) else 'Not a tensor' for factor in factors]}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaW_S-_12i-Y",
        "outputId": "82906997-a886-46be-9222-0dd1b0921bc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Core shape: Not a tensor\n",
            "Factor shapes: [torch.Size([]), torch.Size([]), torch.Size([]), torch.Size([]), torch.Size([]), torch.Size([])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W = layer.weight.data.numpy()  # Перетворення на numpy для використання в TensorLy\n",
        "\n",
        "    # Виконання часткового Tucker-розкладу з двома модами\n",
        "core, factors = partial_tucker(W, modes=[0, 1], rank=10, init='svd')\n",
        "\n",
        "    # Отримання двох факторів\n",
        "last, first = factors  # Порядок залежить від partial_tucker\n",
        "\n",
        "    # Перевірка розмірів факторів\n",
        "print(f\"first shape: {first.shape}\")\n",
        "print(f\"last shape: {last.shape}\")\n",
        "print(f\"core shape: {core.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "xeiMTRmcpxOU",
        "outputId": "3e9f3e24-4aba-47f7-b682-1cc7dd50c6bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "moveaxis() received an invalid combination of arguments - got (numpy.ndarray, int, int), but expected one of:\n * (Tensor input, int source, int destination)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, int, int)\n * (Tensor input, tuple of ints source, tuple of ints destination)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !int!, !int!)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-cc3be75f72b4>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Виконання часткового Tucker-розкладу з двома модами\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial_tucker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'svd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Отримання двох факторів\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorly/decomposition/_tucker.py\u001b[0m in \u001b[0;36mpartial_tucker\u001b[0;34m(tensor, rank, modes, n_iter_max, init, tol, svd, random_state, verbose, mask, svd_mask_repeats)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;31m# SVD init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     core, factors = initialize_tucker(\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorly/decomposition/_tucker.py\u001b[0m in \u001b[0;36minitialize_tucker\u001b[0;34m(tensor, rank, modes, random_state, init, svd, non_negative, mask, svd_mask_repeats)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mmask_unfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0munfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             U, _, _ = svd_interface(\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0munfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0mn_eigenvecs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorly/base.py\u001b[0m in \u001b[0;36munfold\u001b[0;34m(tensor, mode)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0munfolded_tensor\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \"\"\"\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorly/backend/__init__.py\u001b[0m in \u001b[0;36mwrapped_backend_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             Returns the queried method from the currently set backend\"\"\"\n\u001b[0;32m--> 206\u001b[0;31m             return getattr(\n\u001b[0m\u001b[1;32m    207\u001b[0m                 \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_THREAD_LOCAL_DATA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"backend\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             )(*args, **kwargs)\n",
            "\u001b[0;31mTypeError\u001b[0m: moveaxis() received an invalid combination of arguments - got (numpy.ndarray, int, int), but expected one of:\n * (Tensor input, int source, int destination)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, int, int)\n * (Tensor input, tuple of ints source, tuple of ints destination)\n      didn't match because some of the arguments have invalid types: (!numpy.ndarray!, !int!, !int!)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Factors: {factors}\")\n",
        "\n",
        "# Перевірка форми кожного фактора\n",
        "for i, factor in enumerate(factors):\n",
        "    print(f\"Factor {i} shape: {factor.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbBi5lILp2q9",
        "outputId": "933a9b61-a75a-4e46-e21e-a6a0db23be1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Factors: [tensor(0.7286), tensor(0.7283), tensor(0.7283)]\n",
            "Factor 0 shape: torch.Size([])\n",
            "Factor 1 shape: torch.Size([])\n",
            "Factor 2 shape: torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"W shape: {W.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UWYsxS4uHK4",
        "outputId": "851f5b3c-25ba-446a-b4bb-ecf91935530b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W shape: torch.Size([64, 32, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SsJxljWCuHjc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}